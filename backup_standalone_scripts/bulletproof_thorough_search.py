#!/usr/bin/env python3
"""
Bulletproof wrapper for thorough_search hyperparameter optimization.
Provides comprehensive safeguards, monitoring, and recovery for 8+ hour runs.

This script ensures:
1. Pre-flight validation passes before starting
2. Automatic checkpoint saving and recovery
3. Resource monitoring and alerts
4. Error handling and graceful degradation
5. Model validation after optimization
6. Integration testing with inference pipeline

Usage:
    python bulletproof_thorough_search.py [--resume] [--validate-only] [--dry-run]
"""

import os
import sys
import json
import time
import signal
import argparse
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any

# Add project root to path
sys.path.insert(0, os.path.abspath('.'))

from src.optimization.main import OptimizationOrchestrator
from src.inference_pipeline import run_inference
from src.config_legacy import CONFIG


class BulletproofThoroughSearch:
    """Bulletproof wrapper for thorough_search optimization."""
    
    def __init__(self, output_dir: str = "thorough_search_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize components
        self.orchestrator: Optional[OptimizationOrchestrator] = None
        self.start_time: Optional[datetime] = None
        self.interrupted = False
        
        # Results tracking
        self.results = {
            'start_time': None,
            'end_time': None,
            'status': 'initializing',
            'trials_completed': 0,
            'best_score': None,
            'best_parameters': None,
            'errors': [],
            'checkpoints_saved': 0,
            'model_validation_passed': False,
            'inference_validation_passed': False
        }
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Handle interruption signals gracefully."""
        print(f"\n‚ö†Ô∏è  Received signal {signum}. Initiating graceful shutdown...")
        self.interrupted = True
    
    def pre_flight_validation(self) -> bool:
        """Run comprehensive pre-flight validation."""
        print("üîç PRE-FLIGHT VALIDATION")
        print("=" * 50)
        
        validation_checks = [
            self._validate_environment,
            self._validate_data_file,
            self._validate_disk_space,
            self._validate_optimization_setup,
            self._validate_model_compatibility,
            self._validate_inference_pipeline
        ]
        
        passed_checks = 0
        total_checks = len(validation_checks)
        
        for i, check in enumerate(validation_checks, 1):
            try:
                print(f"\nüìã Check {i}/{total_checks}: {check.__name__.replace('_validate_', '').replace('_', ' ').title()}")
                if check():
                    print("   ‚úÖ PASSED")
                    passed_checks += 1
                else:
                    print("   ‚ùå FAILED")
            except Exception as e:
                print(f"   ‚ùå ERROR: {e}")
        
        success_rate = passed_checks / total_checks
        print(f"\nüìä VALIDATION SUMMARY")
        print(f"   Passed: {passed_checks}/{total_checks} ({success_rate:.1%})")
        
        if success_rate == 1.0:
            print("   üéØ ALL CHECKS PASSED - Ready for thorough_search!")
            return True
        elif success_rate >= 0.8:
            print("   ‚ö†Ô∏è  MOSTLY PASSED - Proceed with caution")
            return self._get_user_confirmation()
        else:
            print("   ‚ùå VALIDATION FAILED - Do not proceed!")
            return False
    
    def _validate_environment(self) -> bool:
        """Validate environment setup."""
        try:
            import torch
            device_available = torch.cuda.is_available()
            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if device_available else 0
            
            print(f"      PyTorch: {torch.__version__}")
            print(f"      CUDA: {'Available' if device_available else 'Not available'}")
            if device_available:
                print(f"      GPU Memory: {memory_gb:.1f} GB")
            
            return True
        except Exception as e:
            print(f"      Error: {e}")
            return False
    
    def _validate_data_file(self) -> bool:
        """Validate data file exists and is readable."""
        try:
            data_path = CONFIG['data_path']
            if not os.path.exists(data_path):
                print(f"      Data file missing: {data_path}")
                return False
            
            # Try to read a few lines
            import pandas as pd
            df = pd.read_csv(data_path, nrows=5)
            print(f"      Data file: {data_path} ({len(df)} sample rows)")
            return True
        except Exception as e:
            print(f"      Error reading data: {e}")
            return False
    
    def _validate_disk_space(self) -> bool:
        """Validate sufficient disk space."""
        try:
            import shutil
            free_space_gb = shutil.disk_usage('.').free / 1e9
            required_space_gb = 5.0  # Minimum 5GB required
            
            print(f"      Free space: {free_space_gb:.1f} GB")
            print(f"      Required: {required_space_gb:.1f} GB")
            
            if free_space_gb < required_space_gb:
                print(f"      Insufficient disk space!")
                return False
            
            return True
        except Exception as e:
            print(f"      Error checking disk space: {e}")
            return False
    
    def _validate_optimization_setup(self) -> bool:
        """Validate optimization components."""
        try:
            # Test orchestrator creation
            orchestrator = OptimizationOrchestrator(CONFIG['data_path'], str(self.output_dir))
            
            # Test presets
            presets = orchestrator.list_presets()
            if 'thorough_search' not in presets:
                print("      thorough_search preset missing!")
                return False
            
            # Test preset info
            preset_info = orchestrator.get_preset_info('thorough_search')
            print(f"      Preset: {preset_info['algorithm']}, {preset_info['max_trials']} trials, {preset_info['max_duration_hours']}h")
            
            return True
        except Exception as e:
            print(f"      Error: {e}")
            return False
    
    def _validate_model_compatibility(self) -> bool:
        """Validate model classes can be instantiated."""
        try:
            from src.cvae_model import ConditionalVAE
            from src.meta_learner import AttentionMetaLearner
            
            # Test model creation with default config
            test_config = CONFIG.copy()
            test_config.update({'hidden_size': 64, 'latent_dim': 32})
            
            cvae = ConditionalVAE(test_config)
            meta = AttentionMetaLearner(test_config)
            
            print(f"      CVAE: {cvae.__class__.__name__}")
            print(f"      Meta-learner: {meta.__class__.__name__}")
            
            return True
        except Exception as e:
            print(f"      Error: {e}")
            return False
    
    def _validate_inference_pipeline(self) -> bool:
        """Validate inference pipeline components."""
        try:
            # Test imports
            from src.inference_pipeline import GenerativeEnsemble
            from src.temporal_scorer import TemporalScorer
            from src.i_ching_scorer import IChingScorer
            
            print(f"      Inference components available")
            return True
        except Exception as e:
            print(f"      Error: {e}")
            return False
    
    def _get_user_confirmation(self) -> bool:
        """Get user confirmation to proceed despite warnings."""
        try:
            response = input("\n‚ùì Some validation checks failed. Proceed anyway? (y/N): ")
            return response.lower() == 'y'
        except (EOFError, KeyboardInterrupt):
            return False
    
    def run_thorough_search(self, resume: bool = False, dry_run: bool = False) -> Dict[str, Any]:
        """Run the thorough_search optimization with full monitoring."""
        print("\nüöÄ STARTING THOROUGH SEARCH OPTIMIZATION")
        print("=" * 60)
        
        if dry_run:
            print("üß™ DRY RUN MODE - No actual optimization will be performed")
            return {'status': 'dry_run_completed'}
        
        self.start_time = datetime.now()
        self.results['start_time'] = self.start_time.isoformat()
        self.results['status'] = 'running'
        
        try:
            # Initialize orchestrator
            print("üìã Initializing optimization orchestrator...")
            self.orchestrator = OptimizationOrchestrator(
                data_path=CONFIG['data_path'],
                output_dir=str(self.output_dir)
            )
            
            # Check for resume opportunity
            if resume:
                print("üîÑ Checking for existing checkpoints...")
                # Implementation would check for checkpoints here
                
            # Configure thorough_search
            print("‚öôÔ∏è  Configuring thorough_search parameters...")
            optimization_config = {
                'preset_name': 'thorough_search',
                'max_trials': None,  # Use preset default (50)
                'max_duration_hours': None  # Use preset default (8.0)
            }
            
            print(f"   Algorithm: Optuna TPE")
            print(f"   Max trials: 50")
            print(f"   Max duration: 8.0 hours")
            print(f"   Expected completion: {(self.start_time + timedelta(hours=8.0)).strftime('%Y-%m-%d %H:%M:%S')}")
            
            # Start optimization with monitoring
            print("\nüéØ Starting optimization (this will take 8+ hours)...")
            print("   Press Ctrl+C for graceful shutdown with checkpoint saving")
            
            # Save initial status
            self._save_status_report()
            
            # Run optimization
            results = self.orchestrator.run_optimization(**optimization_config)
            
            # Update results
            self.results['status'] = 'completed'
            self.results['end_time'] = datetime.now().isoformat()
            
            if results and 'optimization_summary' in results:
                summary = results['optimization_summary']
                self.results['trials_completed'] = summary.get('total_trials', 0)
                self.results['best_score'] = summary.get('best_score')
                self.results['best_parameters'] = summary.get('best_parameters')
            
            print(f"\nüéâ OPTIMIZATION COMPLETED!")
            print(f"   Duration: {datetime.now() - self.start_time}")
            print(f"   Best score: {self.results['best_score']}")
            
            # Post-optimization validation
            self._post_optimization_validation()
            
            return self.results
            
        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è  Optimization interrupted by user")
            self.results['status'] = 'interrupted'
            self.results['end_time'] = datetime.now().isoformat()
            return self.results
            
        except Exception as e:
            print(f"\n‚ùå Optimization failed: {e}")
            self.results['status'] = 'failed'
            self.results['end_time'] = datetime.now().isoformat()
            self.results['errors'].append(str(e))
            traceback.print_exc()
            return self.results
        
        finally:
            # Always save final status
            self._save_status_report()
    
    def _post_optimization_validation(self):
        """Validate optimization results and model compatibility."""
        print("\nüîç POST-OPTIMIZATION VALIDATION")
        print("-" * 40)
        
        try:
            # Copy best models to standard locations for inference compatibility
            self._copy_optimized_models_for_inference()
            
            # Check if models were saved
            model_paths = [
                str(self.output_dir / "best_cvae_model.pth"),
                str(self.output_dir / "best_meta_learner.pth"), 
                str(self.output_dir / "best_feature_engineer.pkl"),
                "thorough_search_results/best_cvae_model.pth",  # Check copied models
                CONFIG.get('model_save_path'),
                CONFIG.get('meta_learner_save_path'),
                CONFIG.get('feature_engineer_path')
            ]
            
            models_found = 0
            for path in model_paths:
                if path and os.path.exists(path):
                    models_found += 1
                    print(f"   ‚úÖ Found: {path}")
                else:
                    print(f"   ‚ö†Ô∏è  Missing: {path}")
            
            self.results['model_validation_passed'] = models_found >= 3
            
            # Test inference compatibility
            if models_found >= 2:
                print("\n   Testing inference compatibility...")
                try:
                    # Quick inference test
                    recommendations, _ = run_inference(
                        num_sets_to_generate=1,
                        use_i_ching=False,
                        temperature=0.8,
                        verbose=False
                    )
                    
                    if recommendations and len(recommendations) == 1:
                        print("   ‚úÖ Inference test passed")
                        self.results['inference_validation_passed'] = True
                    else:
                        print("   ‚ùå Inference test failed")
                        
                except Exception as e:
                    print(f"   ‚ùå Inference test error: {e}")
            
        except Exception as e:
            print(f"   ‚ùå Validation error: {e}")
            self.results['errors'].append(f"Post-validation error: {e}")
    
    def _copy_optimized_models_for_inference(self):
        """Copy optimized models to locations where inference pipeline can find them."""
        try:
            import shutil
            
            # Map of source patterns to target paths
            copy_mappings = [
                # Look for best models in optimization results
                (self.output_dir / "best_parameters.json", "thorough_search_results/best_parameters.json"),
                
                # Look for model files in optimization output
                (self.output_dir / "*cvae*.pth", "thorough_search_results/best_cvae_model.pth"),
                (self.output_dir / "*meta*.pth", "thorough_search_results/best_meta_learner.pth"),
                (self.output_dir / "*feature*.pkl", "thorough_search_results/best_feature_engineer.pkl"),
                
                # Also check models/ directory for recently created models
                ("models/conservative_cvae_model.pth", "thorough_search_results/best_cvae_model.pth"),
                ("models/conservative_meta_learner.pth", "thorough_search_results/best_meta_learner.pth"),
                ("models/conservative_feature_engineer.pkl", "thorough_search_results/best_feature_engineer.pkl"),
            ]
            
            print("   üìÅ Copying optimized models for inference compatibility...")
            
            # Create thorough_search_results directory if it doesn't exist
            Path("thorough_search_results").mkdir(exist_ok=True)
            
            for source_pattern, target_path in copy_mappings:
                # Handle glob patterns
                if "*" in str(source_pattern):
                    import glob
                    matching_files = glob.glob(str(source_pattern))
                    if matching_files:
                        # Use the most recently modified file
                        source_file = max(matching_files, key=os.path.getmtime)
                        if os.path.exists(source_file):
                            shutil.copy2(source_file, target_path)
                            print(f"   üìÑ Copied: {source_file} ‚Üí {target_path}")
                else:
                    # Direct file copy
                    if os.path.exists(source_pattern):
                        shutil.copy2(source_pattern, target_path)
                        print(f"   üìÑ Copied: {source_pattern} ‚Üí {target_path}")
            
            # Create a marker file indicating thorough_search completion
            marker_file = Path("thorough_search_results/optimization_completed.txt")
            with open(marker_file, 'w') as f:
                f.write(f"Thorough search optimization completed at: {datetime.now().isoformat()}\n")
                f.write(f"Best score: {self.results.get('best_score', 'N/A')}\n")
                f.write(f"Models are ready for inference via main.py option 2\n")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error copying models: {e}")
    
    def _save_status_report(self):
        """Save current status to file."""
        try:
            status_file = self.output_dir / "optimization_status.json"
            
            # Add runtime info
            if self.start_time:
                self.results['runtime_hours'] = (datetime.now() - self.start_time).total_seconds() / 3600
            
            with open(status_file, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
                
        except Exception as e:
            print(f"Warning: Could not save status report: {e}")
    
    def get_status_report(self) -> Dict[str, Any]:
        """Get current status report."""
        return self.results.copy()


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Bulletproof thorough_search optimization")
    parser.add_argument('--resume', action='store_true', 
                       help='Resume from latest checkpoint')
    parser.add_argument('--validate-only', action='store_true',
                       help='Run only pre-flight validation')
    parser.add_argument('--dry-run', action='store_true',
                       help='Validate and setup but do not run optimization')
    parser.add_argument('--output-dir', default='thorough_search_results',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    # Initialize bulletproof wrapper
    bulletproof = BulletproofThoroughSearch(args.output_dir)
    
    try:
        print("üõ°Ô∏è  BULLETPROOF THOROUGH SEARCH")
        print("=" * 60)
        print("This wrapper provides comprehensive safeguards for 8+ hour optimization runs")
        print("Features: Pre-flight validation, checkpointing, monitoring, error recovery")
        print("=" * 60)
        
        # Run pre-flight validation
        if not bulletproof.pre_flight_validation():
            print("\n‚ùå PRE-FLIGHT VALIDATION FAILED")
            print("Fix the issues above before running thorough_search!")
            return 1
        
        if args.validate_only:
            print("\n‚úÖ VALIDATION ONLY MODE - All checks passed!")
            print("You can now run thorough_search with confidence.")
            return 0
        
        # Confirm before starting
        if not args.dry_run:
            print(f"\n‚ùì Ready to start 8+ hour thorough_search optimization.")
            print(f"   Results will be saved to: {args.output_dir}")
            print(f"   Expected completion: {(datetime.now() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')}")
            
            try:
                confirm = input("\n   Proceed? (y/N): ")
                if confirm.lower() != 'y':
                    print("Cancelled by user.")
                    return 0
            except (EOFError, KeyboardInterrupt):
                print("\nCancelled by user.")
                return 0
        
        # Run thorough_search
        results = bulletproof.run_thorough_search(
            resume=args.resume,
            dry_run=args.dry_run
        )
        
        # Display final results
        print(f"\nüèÅ FINAL RESULTS")
        print(f"   Status: {results['status']}")
        print(f"   Trials completed: {results.get('trials_completed', 0)}")
        if results.get('best_score'):
            print(f"   Best score: {results['best_score']:.6f}")
        if results.get('runtime_hours'):
            print(f"   Runtime: {results['runtime_hours']:.2f} hours")
        
        if results['status'] == 'completed':
            print(f"\nüéâ SUCCESS! Optimization completed successfully.")
            if results.get('model_validation_passed') and results.get('inference_validation_passed'):
                print(f"   ‚úÖ Models are ready for inference!")
            else:
                print(f"   ‚ö†Ô∏è  Some post-validation checks failed.")
            return 0
        elif results['status'] == 'dry_run_completed':
            print(f"\nüß™ DRY RUN COMPLETED! Setup validation successful.")
            print(f"   Ready to run actual optimization.")
            return 0
        else:
            print(f"\n‚ö†Ô∏è  Optimization did not complete normally.")
            print(f"   Check the logs and status report for details.")
            return 1
            
    except KeyboardInterrupt:
        print(f"\n\nOperation cancelled by user.")
        return 1
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)