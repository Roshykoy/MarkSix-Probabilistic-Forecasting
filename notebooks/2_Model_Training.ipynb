{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 2: CVAE Model Training Pipeline\n\nThis notebook provides an interactive walkthrough of the new CVAE-based model training process. It demonstrates the advanced architecture with graph neural networks, temporal context modeling, and meta-learning capabilities. The training process uses the enhanced training pipeline from `src/training_pipeline.py` with conservative configuration for stability.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport joblib\nfrom tqdm.notebook import tqdm # Use notebook-friendly tqdm\nimport random\nimport matplotlib.pyplot as plt\n\n# Add the source directory to the Python path\nsys.path.append(os.path.abspath(os.path.join('..')))\n\n# Import from our project's new CVAE-based architecture\nfrom src.config import CONFIG\nfrom src.cvae_model import CVAEModel\nfrom src.cvae_engine import CVAETrainingEngine\nfrom src.cvae_data_loader import CVAEDataLoader\nfrom src.feature_engineering import FeatureEngineer\nfrom src.graph_encoder import GraphEncoder\nfrom src.temporal_context import TemporalContextModel\nfrom src.meta_learner import MetaLearner\nfrom src.debug_utils import setup_debug_logging, log_debug_info\n\nprint(\"Setup complete. New CVAE modules loaded.\")\nprint(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\nprint(f\"Using conservative training configuration for stability.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2. Initialize Debug Logging and Load Data\n\nThe new architecture includes comprehensive debugging utilities to monitor training stability and catch potential issues early.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup debug logging for comprehensive monitoring\ndebug_logger = setup_debug_logging(\"notebook_training\")\nlog_debug_info(\"Starting CVAE training notebook\", debug_logger)\n\n# Load and prepare data\ncol_names = [\n    'Draw', 'Date', 'Winning_Num_1', 'Winning_Num_2', 'Winning_Num_3',\n    'Winning_Num_4', 'Winning_Num_5', 'Winning_Num_6', 'Extra_Num',\n    'From_Last', 'Low', 'High', 'Odd', 'Even', '1-10', '11-20', '21-30',\n    '31-40', '41-50', 'Div_1_Winners', 'Div_1_Prize', 'Div_2_Winners',\n    'Div_2_Prize', 'Div_3_Winners', 'Div_3_Prize', 'Div_4_Winners',\n    'Div_4_Prize', 'Div_5_Winners', 'Div_5_Prize', 'Div_6_Winners',\n    'Div_6_Prize', 'Div_7_Winners', 'Div_7_Prize', 'Turnover'\n]\n\ndata_path = os.path.join('..', CONFIG[\"data_path\"])\ndf = pd.read_csv(data_path, header=None, skiprows=33, names=col_names)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.sort_values(by='Date').reset_index(drop=True)\n\nprint(f\"Loaded {len(df)} historical draws\")\nprint(f\"Data range: {df['Date'].min()} to {df['Date'].max()}\")\n\n# Initialize and fit the enhanced feature engineer\nfeature_engineer = FeatureEngineer()\nfeature_engineer.fit(df)\nprint(f\"Feature engineer fitted. Feature dimension: {feature_engineer.get_feature_dim()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3. Initialize CVAE Architecture Components\n\nThe new architecture consists of multiple components working together: CVAE core, graph encoder, temporal context, and meta-learner.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup device and configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize CVAE data loader with negative sampling\ncvae_data_loader = CVAEDataLoader(CONFIG)\ntrain_loader, val_loader = cvae_data_loader.create_data_loaders(df, feature_engineer)\n\nprint(f\"Created data loaders:\")\nprint(f\"  - Training batches: {len(train_loader)}\")\nprint(f\"  - Validation batches: {len(val_loader)}\")\nprint(f\"  - Batch size: {CONFIG['batch_size']}\")\n\n# Initialize graph encoder for number relationships\ngraph_encoder = GraphEncoder(\n    num_nodes=CONFIG['num_lotto_numbers'],\n    input_dim=CONFIG['graph_input_dim'],\n    hidden_dim=CONFIG['graph_hidden_dim'],\n    output_dim=CONFIG['graph_output_dim']\n).to(device)\n\n# Initialize temporal context model\ntemporal_model = TemporalContextModel(\n    input_dim=CONFIG['temporal_input_dim'],\n    hidden_dim=CONFIG['temporal_hidden_dim'],\n    output_dim=CONFIG['temporal_output_dim']\n).to(device)\n\n# Initialize meta-learner for ensemble optimization\nmeta_learner = MetaLearner(\n    input_dim=CONFIG['meta_input_dim'],\n    hidden_dim=CONFIG['meta_hidden_dim'],\n    num_scorers=CONFIG['num_ensemble_scorers']\n).to(device)\n\n# Initialize main CVAE model\ncvae_model = CVAEModel(\n    feature_dim=feature_engineer.get_feature_dim(),\n    latent_dim=CONFIG['cvae_latent_dim'],\n    hidden_dim=CONFIG['cvae_hidden_dim'],\n    graph_encoder=graph_encoder,\n    temporal_encoder=temporal_model,\n    meta_learner=meta_learner\n).to(device)\n\nprint(f\"CVAE model initialized with {sum(p.numel() for p in cvae_model.parameters())} parameters\")\nprint(f\"Conservative configuration: latent_dim={CONFIG['cvae_latent_dim']}, hidden_dim={CONFIG['cvae_hidden_dim']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4. Initialize Training Engine and Optimizer\n\nThe new training engine includes advanced features like mixed precision handling, gradient clipping, and error recovery.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize training engine with conservative settings\ntraining_engine = CVAETrainingEngine(\n    model=cvae_model,\n    config=CONFIG,\n    device=device,\n    logger=debug_logger\n)\n\n# Setup optimizer with conservative learning rate\noptimizer = torch.optim.AdamW(\n    cvae_model.parameters(),\n    lr=CONFIG['learning_rate'],  # Conservative: 5e-5\n    weight_decay=CONFIG['weight_decay']\n)\n\nprint(f\"Training engine initialized with:\")\nprint(f\"  - Learning rate: {CONFIG['learning_rate']}\")\nprint(f\"  - Weight decay: {CONFIG['weight_decay']}\")\nprint(f\"  - Gradient clipping: {CONFIG['gradient_clip_norm']}\")\nprint(f\"  - Mixed precision: {CONFIG['use_mixed_precision']}\")\nprint(f\"  - Conservative training: {CONFIG['conservative_training']}\")\n\n# Initialize loss tracking\ntrain_losses = []\nval_losses = []\nreconstruction_losses = []\nkl_losses = []\ncontrastive_losses = []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5. Run CVAE Training Loop\n\nThe training loop includes comprehensive monitoring, error handling, and stability checks. The multi-component loss combines reconstruction, KL divergence, and contrastive learning.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\nStarting CVAE training for {CONFIG['epochs']} epochs...\")\nprint(\"=\" * 60)\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\nfor epoch in range(CONFIG['epochs']):\n    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n    print(\"-\" * 40)\n    \n    # Training phase\n    train_metrics = training_engine.train_epoch(train_loader, optimizer, epoch)\n    \n    # Validation phase\n    val_metrics = training_engine.validate_epoch(val_loader, epoch)\n    \n    # Track losses\n    train_losses.append(train_metrics['total_loss'])\n    val_losses.append(val_metrics['total_loss'])\n    reconstruction_losses.append(train_metrics['reconstruction_loss'])\n    kl_losses.append(train_metrics['kl_loss'])\n    contrastive_losses.append(train_metrics['contrastive_loss'])\n    \n    # Print metrics\n    print(f\"Train Loss: {train_metrics['total_loss']:.4f} \"\n          f\"(Recon: {train_metrics['reconstruction_loss']:.4f}, \"\n          f\"KL: {train_metrics['kl_loss']:.4f}, \"\n          f\"Contrastive: {train_metrics['contrastive_loss']:.4f})\")\n    \n    print(f\"Val Loss: {val_metrics['total_loss']:.4f} \"\n          f\"(Recon: {val_metrics['reconstruction_loss']:.4f}, \"\n          f\"KL: {val_metrics['kl_loss']:.4f}, \"\n          f\"Contrastive: {val_metrics['contrastive_loss']:.4f})\")\n    \n    # Early stopping check\n    if val_metrics['total_loss'] < best_val_loss:\n        best_val_loss = val_metrics['total_loss']\n        patience_counter = 0\n        print(\"âœ“ New best validation loss - saving checkpoint\")\n    else:\n        patience_counter += 1\n        if patience_counter >= CONFIG['patience']:\n            print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n            break\n    \n    # Memory cleanup\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(\"\\nTraining completed!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6. Visualize Training Progress\n\nLet's plot the training curves to understand how the different loss components evolved during training.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Total loss\naxes[0, 0].plot(train_losses, label='Training', color='blue')\naxes[0, 0].plot(val_losses, label='Validation', color='red')\naxes[0, 0].set_title('Total Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# Reconstruction loss\naxes[0, 1].plot(reconstruction_losses, label='Reconstruction', color='green')\naxes[0, 1].set_title('Reconstruction Loss')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# KL divergence loss\naxes[1, 0].plot(kl_losses, label='KL Divergence', color='orange')\naxes[1, 0].set_title('KL Divergence Loss')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Contrastive loss\naxes[1, 1].plot(contrastive_losses, label='Contrastive', color='purple')\naxes[1, 1].set_title('Contrastive Loss')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Loss')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Training visualization complete.\")\nprint(f\"Final training loss: {train_losses[-1]:.4f}\")\nprint(f\"Final validation loss: {val_losses[-1]:.4f}\")\n\n# Save training artifacts\nprint(\"\\nSaving CVAE model and feature engineer...\")\nmodel_dir = os.path.join('..', 'models')\nos.makedirs(model_dir, exist_ok=True)\n\n# Save with conservative prefix\nmodel_save_path = os.path.join(model_dir, 'conservative_cvae_model.pth')\nfe_save_path = os.path.join(model_dir, 'conservative_feature_engineer.pkl')\n\ntorch.save(cvae_model.state_dict(), model_save_path)\njoblib.dump(feature_engineer, fe_save_path)\n\nprint(f\"CVAE model saved to: {model_save_path}\")\nprint(f\"Feature engineer saved to: {fe_save_path}\")\nprint(\"Training notebook completed successfully!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}