{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Model Training Pipeline\n",
    "\n",
    "This notebook provides an interactive walkthrough of the model training process. It replicates the logic from `src/training_pipeline.py`, allowing you to execute each step individually and inspect the components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm # Use notebook-friendly tqdm\n",
    "import random\n",
    "\n",
    "# Add the source directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import from our project's source files\n",
    "from src.config import CONFIG\n",
    "from src.model import ScoringModel\n",
    "from src.engine import train_one_epoch, evaluate\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "from src.sam import SAM\n",
    "\n",
    "print(\"Setup complete. Modules loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Helper Classes and Functions\n",
    "\n",
    "We'll define the `ContrastiveDataset` and the `build_negative_pool` function, which are essential for creating the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_negative_pool(df_all_draws, pool_size, config):\n",
    "    print(f\"Building a pool of {pool_size} negative samples...\")\n",
    "    winning_num_cols = [f'Winning_Num_{i}' for i in range(1, 7)]\n",
    "    historical_sets = {tuple(sorted(draw)) for draw in df_all_draws[winning_num_cols].itertuples(index=False)}\n",
    "    \n",
    "    negative_pool = []\n",
    "    # Using a simple loop here for clarity, tqdm can be added for long processes\n",
    "    while len(negative_pool) < pool_size:\n",
    "        candidate = tuple(sorted(random.sample(range(1, config['num_lotto_numbers'] + 1), 6)))\n",
    "        if candidate not in historical_sets:\n",
    "            negative_pool.append(list(candidate))\n",
    "    print(\"Negative pool built.\")\n",
    "    return negative_pool\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, df, fe, config, negative_pool):\n",
    "        self.df, self.fe, self.config, self.negative_pool = df, fe, config, negative_pool\n",
    "        self.winning_num_cols = [f'Winning_Num_{i}' for i in range(1, 7)]\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        positive_row = self.df.iloc[idx]\n",
    "        positive_set = positive_row[self.winning_num_cols].astype(int).tolist()\n",
    "        positive_features = self.fe.transform(positive_set, idx)\n",
    "        neg_indices = np.random.choice(len(self.negative_pool), self.config['negative_samples'], replace=False)\n",
    "        negative_features = np.array([self.fe.transform(self.negative_pool[i], idx) for i in neg_indices])\n",
    "        \n",
    "        return {\"positive_features\": torch.tensor(positive_features, dtype=torch.float32),\n",
    "                \"negative_features\": torch.tensor(negative_features, dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Data and Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'Draw', 'Date', 'Winning_Num_1', 'Winning_Num_2', 'Winning_Num_3',\n",
    "    'Winning_Num_4', 'Winning_Num_5', 'Winning_Num_6', 'Extra_Num',\n",
    "    'From_Last', 'Low', 'High', 'Odd', 'Even', '1-10', '11-20', '21-30',\n",
    "    '31-40', '41-50', 'Div_1_Winners', 'Div_1_Prize', 'Div_2_Winners',\n",
    "    'Div_2_Prize', 'Div_3_Winners', 'Div_3_Prize', 'Div_4_Winners',\n",
    "    'Div_4_Prize', 'Div_5_Winners', 'Div_5_Prize', 'Div_6_Winners',\n",
    "    'Div_6_Prize', 'Div_7_Winners', 'Div_7_Prize', 'Turnover'\n",
    "]\n",
    "data_path = os.path.join('..', CONFIG[\"data_path\"])\n",
    "df = pd.read_csv(data_path, header=None, skiprows=33, names=col_names)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_engineer.fit(df)\n",
    "\n",
    "negative_pool = build_negative_pool(df, pool_size=10000, config=CONFIG) # Smaller pool for faster notebook execution\n",
    "\n",
    "train_size = int(len(df) * 0.85)\n",
    "train_df, val_df = df.iloc[:train_size], df.iloc[train_size:]\n",
    "train_dataset = ContrastiveDataset(train_df, feature_engineer, CONFIG, negative_pool)\n",
    "val_dataset = ContrastiveDataset(val_df, feature_engineer, CONFIG, negative_pool)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(\"Data loaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get feature dimension from the engineer\n",
    "sample_features = feature_engineer.transform([1,2,3,4,5,6], 0)\n",
    "CONFIG['d_features'] = len(sample_features)\n",
    "\n",
    "model = ScoringModel(CONFIG).to(device)\n",
    "\n",
    "if CONFIG['use_sam_optimizer']:\n",
    "    print(\"Using Sharpness-Aware Minimization (SAM) optimizer.\")\n",
    "    base_optimizer = optim.AdamW\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=CONFIG[\"learning_rate\"], rho=CONFIG['rho'])\n",
    "else:\n",
    "    print(\"Using standard AdamW optimizer.\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the Training Loop\n",
    "\n",
    "This loop iterates through the specified number of epochs, calling the training and evaluation engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting contrastive training on {device}...\")\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    avg_train_loss = train_one_epoch(model, train_loader, optimizer, device, CONFIG)\n",
    "    avg_val_loss = evaluate(model, val_loader, device, CONFIG)\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Train Ranking Loss: {avg_train_loss:.4f} | Validation Ranking Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save the Final Artifacts\n",
    "\n",
    "After training is complete, we save the model's state dictionary and the fitted `FeatureEngineer` object. These are the artifacts that will be used by the inference and evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining complete. Saving model and feature engineer...\")\n",
    "\n",
    "# Ensure model directory exists\n",
    "model_dir = os.path.join('..', 'models')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(model_dir, 'scoring_model.pth')\n",
    "fe_save_path = os.path.join(model_dir, 'feature_engineer.pkl')\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "joblib.dump(feature_engineer, fe_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"Feature engineer saved to: {fe_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
