{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Inference and Evaluation\n",
    "\n",
    "This notebook demonstrates the final two stages of the project: generating recommended number sets using the trained ensemble model (inference) and evaluating the model's performance on unseen historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the source directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import all necessary components from our project\n",
    "from src.config import CONFIG\n",
    "from src.model import ScoringModel\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "from src.temporal_scorer import TemporalScorer\n",
    "from src.i_ching_scorer import IChingScorer\n",
    "from src.inference_pipeline import ScorerEnsemble, local_search\n",
    "\n",
    "print(\"Setup complete. Modules loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Trained Artifacts and Data\n",
    "\n",
    "Before we can generate or evaluate, we need to load the saved `ScoringModel` and the fitted `FeatureEngineer`. We also load the historical data to initialize the heuristic scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model artifacts exist\n",
    "model_path = os.path.join('..', CONFIG[\"model_save_path\"])\n",
    "fe_path = os.path.join('..', CONFIG[\"feature_engineer_path\"])\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(fe_path):\n",
    "    print(\"Model artifacts not found! Please run the training pipeline first.\")\n",
    "else:\n",
    "    # Load artifacts\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    CONFIG['device'] = device\n",
    "\n",
    "    feature_engineer = joblib.load(fe_path)\n",
    "    CONFIG['d_features'] = len(feature_engineer.transform([1,2,3,4,5,6], 0))\n",
    "\n",
    "    model = ScoringModel(CONFIG).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load data for scorers\n",
    "    col_names = [\n",
    "        'Draw', 'Date', 'Winning_Num_1', 'Winning_Num_2', 'Winning_Num_3',\n",
    "        'Winning_Num_4', 'Winning_Num_5', 'Winning_Num_6', 'Extra_Num',\n",
    "        'From_Last', 'Low', 'High', 'Odd', 'Even', '1-10', '11-20', '21-30',\n",
    "        '31-40', '41-50', 'Div_1_Winners', 'Div_1_Prize', 'Div_2_Winners',\n",
    "        'Div_2_Prize', 'Div_3_Winners', 'Div_3_Prize', 'Div_4_Winners',\n",
    "        'Div_4_Prize', 'Div_5_Winners', 'Div_5_Prize', 'Div_6_Winners',\n",
    "        'Div_6_Prize', 'Div_7_Winners', 'Div_7_Prize', 'Turnover'\n",
    "    ]\n",
    "    data_path = os.path.join('..', CONFIG[\"data_path\"])\n",
    "    df = pd.read_csv(data_path, header=None, skiprows=33, names=col_names)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    print(\"Model, FeatureEngineer, and data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference: Generating Number Sets\n",
    "\n",
    "Here, we'll initialize the full `ScorerEnsemble` and use the `local_search` algorithm to generate a few recommended sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the heuristic scorers\n",
    "temporal_scorer = TemporalScorer(CONFIG)\n",
    "temporal_scorer.fit(df)\n",
    "i_ching_scorer = IChingScorer(CONFIG)\n",
    "\n",
    "# --- GENERATE WITHOUT I-CHING ---\n",
    "print(\"--- Generating with DL + Temporal Ensemble ---\")\n",
    "ensemble_no_iching = ScorerEnsemble(model, feature_engineer, temporal_scorer, i_ching_scorer, df, CONFIG, use_i_ching=False)\n",
    "initial_set_1 = sorted(random.sample(range(1, CONFIG['num_lotto_numbers'] + 1), 6))\n",
    "best_set_1, best_score_1 = local_search(\n",
    "    initial_set_1, ensemble_no_iching, \n",
    "    max_iterations=CONFIG['search_iterations'],\n",
    "    num_neighbors=CONFIG['search_neighbors']\n",
    ")\n",
    "print(f\"Generated Set: {best_set_1} (Score: {best_score_1:.4f})\\n\")\n",
    "\n",
    "# --- GENERATE WITH I-CHING ---\n",
    "print(\"--- Generating with DL + Temporal + I-Ching Ensemble ---\")\n",
    "ensemble_with_iching = ScorerEnsemble(model, feature_engineer, temporal_scorer, i_ching_scorer, df, CONFIG, use_i_ching=True)\n",
    "initial_set_2 = sorted(random.sample(range(1, CONFIG['num_lotto_numbers'] + 1), 6))\n",
    "best_set_2, best_score_2 = local_search(\n",
    "    initial_set_2, ensemble_with_iching, \n",
    "    max_iterations=CONFIG['search_iterations'],\n",
    "    num_neighbors=CONFIG['search_neighbors']\n",
    ")\n",
    "print(f\"Generated Set: {best_set_2} (Score: {best_score_2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation: Measuring Model Performance\n",
    "\n",
    "Finally, we run the evaluation logic to see how well our model performs on the validation set. We calculate the \"win rate\" â€” the percentage of times the model scores a real winning set higher than a random set. A score above 50% indicates positive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same ScorerEnsemble instance from the inference step\n",
    "print(\"Evaluating the model on the validation set...\")\n",
    "\n",
    "# Split data\n",
    "train_size = int(len(df) * 0.85)\n",
    "val_df = df.iloc[train_size:].reset_index()\n",
    "\n",
    "wins = 0\n",
    "total_comparisons = 0\n",
    "winning_num_cols = [f'Winning_Num_{i}' for i in range(1, 7)]\n",
    "\n",
    "# Note: Using a smaller number of negative samples for faster notebook execution\n",
    "evaluation_neg_samples = 19 # Compare against 19 random sets\n",
    "\n",
    "for idx, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Evaluating Draws\"):\n",
    "    positive_set = row[winning_num_cols].astype(int).tolist()\n",
    "    positive_score = ensemble_with_iching.score(positive_set)\n",
    "\n",
    "    for _ in range(evaluation_neg_samples):\n",
    "        while True:\n",
    "            negative_set = sorted(random.sample(range(1, CONFIG['num_lotto_numbers'] + 1), 6))\n",
    "            if tuple(negative_set) not in feature_engineer.historical_sets:\n",
    "                break\n",
    "        \n",
    "        negative_score = ensemble_with_iching.score(negative_set)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            wins += 1\n",
    "        total_comparisons += 1\n",
    "\n",
    "win_rate = (wins / total_comparisons) * 100 if total_comparisons > 0 else 0\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Positive (Winning) sets ranked higher: {wins}/{total_comparisons}\")\n",
    "print(f\"Model Win Rate: {win_rate:.2f}%\")\n",
    "print(\"----------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
